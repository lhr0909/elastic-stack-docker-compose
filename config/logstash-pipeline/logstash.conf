# input section, there is beats input and kafka input, along with others

input {
  beats {  # setting input port for taking beats
    port => 5044
  }
}

# filter section, this is the part where we do the parsing

filter {
  json { # json filter to parse initial filebeat payload, raw logs are in the `message` field
    source => "message"
    add_tag => ["ms_%{somefield}"]  # adding a tag by using a field name, just an example
  }
  # we can add more filters here, and the one below shows an example to parse timestamp fields
  #  inside the winston log's `@timestamp` field
  date {
    match => [ "@timestamp" , "ISO8601" ]
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    # this index name can be modified to be something more specific later on,
    # currently it is set to grab the beats metadata, which returns to be `filebeat` 
    index => "%{[@metadata][beat]}-%{+YYYY.MM.dd}" 
    document_type => "%{[@metadata][type]}" # piping through the type metadata here
    # authentication
    user => elastic
    password => changeme
  }
}